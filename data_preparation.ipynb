{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –°–ò–ó\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —à–∞–≥–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö:\n",
        "1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ\n",
        "2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–µ–¥—Ä–∞–∑–º–µ—Ç–∫–∞\n",
        "3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val\n",
        "4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å\n",
        "project_root = Path('.').resolve()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "print(f\"–ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ\n",
        "\n",
        "–ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤—Å–µ –≤–∏–¥–µ–æ –≤ –ø–∞–ø–∫—É `videos/` –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –±–ª–æ–∫.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def extract_frames_from_video(video_path, output_dir, step=30):\n",
        "    \\\"\\\"\\\"\n",
        "    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞–¥—Ä—ã –∏–∑ –≤–∏–¥–µ–æ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —à–∞–≥–æ–º.\n",
        "    \\\"\\\"\\\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        print(f\"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}\")\n",
        "        return 0\n",
        "    \n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "    video_name = video_path.stem\n",
        "    \n",
        "    print(f\"  –û–±—Ä–∞–±–æ—Ç–∫–∞: {video_path.name}\")\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        if frame_count % step == 0:\n",
        "            frame_filename = output_dir / f\"{video_name}_frame_{saved_count:06d}.jpg\"\n",
        "            cv2.imwrite(str(frame_filename), frame)\n",
        "            saved_count += 1\n",
        "        \n",
        "        frame_count += 1\n",
        "    \n",
        "    cap.release()\n",
        "    print(f\"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–¥—Ä–æ–≤: {saved_count} (–∏–∑ {frame_count} –≤—Å–µ–≥–æ)\")\n",
        "    return saved_count\n",
        "\n",
        "def auto_extract_frames(videos_dir=\"videos\", output_dir=\"data/images/train\", step=30):\n",
        "    videos_path = Path(videos_dir)\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    if not videos_path.exists():\n",
        "        print(f\"‚ùå –ü–∞–ø–∫–∞ '{videos_dir}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
        "        print(f\"–°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É '{videos_dir}' –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—ã.\")\n",
        "        return\n",
        "    \n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.m4v']\n",
        "    video_files = []\n",
        "    for ext in video_extensions:\n",
        "        video_files.extend(videos_path.glob(f\"*{ext}\"))\n",
        "        video_files.extend(videos_path.glob(f\"*{ext.upper()}\"))\n",
        "    \n",
        "    if len(video_files) == 0:\n",
        "        print(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤ –≤ '{videos_dir}'!\")\n",
        "        print(f\"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(video_extensions)}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {len(video_files)}\")\n",
        "    total_saved = 0\n",
        "    \n",
        "    for i, video_file in enumerate(video_files, 1):\n",
        "        print(f\"\\n[{i}/{len(video_files)}] {video_file.name}\")\n",
        "        saved = extract_frames_from_video(video_file, output_path, step)\n",
        "        total_saved += saved\n",
        "    \n",
        "    print(f\"\\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {total_saved} –∫–∞–¥—Ä–æ–≤ –≤ {output_dir}/\")\n",
        "    return total_saved\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤\n",
        "print(\"=== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–ê–î–†–û–í –ò–ó –í–ò–î–ï–û ===\")\n",
        "extracted = auto_extract_frames()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –®–∞–≥ 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–µ–¥—Ä–∞–∑–º–µ—Ç–∫–∞\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é YOLOv8 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö bounding boxes –≤–æ–∫—Ä—É–≥ –ª—é–¥–µ–π.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def auto_prelabel(images_dir=\"data/images/train\", labels_dir=\"data/labels/train\", conf_threshold=0.3):\n",
        "    \\\"\\\"\\\"\n",
        "    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–µ–¥—Ä–∞–∑–º–µ—Ç–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
        "    \\\"\\\"\\\"\n",
        "    images_path = Path(images_dir)\n",
        "    labels_path = Path(labels_dir)\n",
        "    labels_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
        "    image_files = []\n",
        "    for ext in image_extensions:\n",
        "        image_files.extend(images_path.glob(ext))\n",
        "        image_files.extend(images_path.glob(ext.upper()))\n",
        "    \n",
        "    if len(image_files) == 0:\n",
        "        print(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {images_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_files)}\")\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ YOLOv8n...\")\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "    \n",
        "    # COCO –∫–ª–∞—Å—Å 'person' = 0\n",
        "    person_class = 0\n",
        "    total_annotations = 0\n",
        "    processed = 0\n",
        "    \n",
        "    for i, image_file in enumerate(image_files, 1):\n",
        "        image = cv2.imread(str(image_file))\n",
        "        if image is None:\n",
        "            continue\n",
        "        \n",
        "        results = model(image, conf=conf_threshold, verbose=False)\n",
        "        boxes = results[0].boxes\n",
        "        \n",
        "        if boxes is None or len(boxes) == 0:\n",
        "            processed += 1\n",
        "            continue\n",
        "        \n",
        "        # –§–∏–ª—å—Ç—Ä—É–µ–º –ª—é–¥–µ–π\n",
        "        person_boxes = []\n",
        "        for box in boxes:\n",
        "            cls = int(box.cls[0].cpu().numpy())\n",
        "            if cls == person_class:\n",
        "                person_boxes.append(box)\n",
        "        \n",
        "        if len(person_boxes) == 0:\n",
        "            processed += 1\n",
        "            continue\n",
        "        \n",
        "        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É\n",
        "        label_file = labels_path / (image_file.stem + \".txt\")\n",
        "        with open(label_file, 'w') as f:\n",
        "            img_h, img_w = image.shape[:2]\n",
        "            for box in person_boxes:\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "                \n",
        "                center_x = (x1 + x2) / 2 / img_w\n",
        "                center_y = (y1 + y2) / 2 / img_h\n",
        "                width = (x2 - x1) / img_w\n",
        "                height = (y2 - y1) / img_h\n",
        "                \n",
        "                # –ö–ª–∞—Å—Å 0 –¥–ª—è –≤—Å–µ—Ö (–ø–æ—Ç–æ–º –≤ LabelImg –ø–æ–º–µ–Ω—è–µ—Ç–µ –Ω–∞ 1 –¥–ª—è –∂–∏–ª–µ—Ç–æ–≤)\n",
        "                f.write(f\"0 {center_x:.6f} {center_y:.6f} {width:.6f} {height:.6f}\\n\")\n",
        "        \n",
        "        total_annotations += len(person_boxes)\n",
        "        processed += 1\n",
        "        if i % 10 == 0:\n",
        "            print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(image_files)}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ –ü—Ä–µ–¥—Ä–∞–∑–º–µ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}/{len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
        "    print(f\"–°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {total_annotations}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {labels_dir}\")\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Ä–∞–∑–º–µ—Ç–∫–∏\n",
        "print(\"=== –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–ï–î–†–ê–ó–ú–ï–¢–ö–ê ===\")\n",
        "auto_prelabel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def split_dataset(train_images_dir, train_labels_dir, val_images_dir, val_labels_dir, val_ratio=0.2, seed=42):\n",
        "    \\\"\\\"\\\"\n",
        "    –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/val.\n",
        "    \\\"\\\"\\\"\n",
        "    random.seed(seed)\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ val\n",
        "    Path(val_images_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(val_labels_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    train_images_path = Path(train_images_dir)\n",
        "    images = list(train_images_path.glob(\"*.jpg\")) + list(train_images_path.glob(\"*.png\"))\n",
        "    \n",
        "    if len(images) == 0:\n",
        "        print(f\"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {train_images_dir}\")\n",
        "        return\n",
        "    \n",
        "    random.shuffle(images)\n",
        "    val_count = int(len(images) * val_ratio)\n",
        "    val_images = images[:val_count]\n",
        "    \n",
        "    print(f\"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}\")\n",
        "    print(f\"Train: {len(images) - val_count} ({(1-val_ratio)*100:.1f}%)\")\n",
        "    print(f\"Val: {val_count} ({val_ratio*100:.1f}%)\")\n",
        "    \n",
        "    moved_count = 0\n",
        "    for img in val_images:\n",
        "        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "        dest_img = Path(val_images_dir) / img.name\n",
        "        shutil.move(str(img), str(dest_img))\n",
        "        \n",
        "        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É\n",
        "        label_file = Path(train_labels_dir) / (img.stem + \".txt\")\n",
        "        if label_file.exists():\n",
        "            dest_label = Path(val_labels_dir) / label_file.name\n",
        "            shutil.move(str(label_file), str(dest_label))\n",
        "            moved_count += 1\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  –ù–µ—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è {img.name}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ –ü–µ—Ä–µ–º–µ—â–µ–Ω–æ {moved_count} –ø–∞—Ä –≤ val/\")\n",
        "    return moved_count\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "print(\"=== –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/VAL ===\")\n",
        "train_images = \"data/images/train\"\n",
        "train_labels = \"data/labels/train\"\n",
        "val_images = \"data/images/val\"\n",
        "val_labels = \"data/labels/val\"\n",
        "\n",
        "split_dataset(train_images, train_labels, val_images, val_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def check_data_structure():\n",
        "    \\\"\\\"\\\"\n",
        "    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ç–∫–∏.\n",
        "    \\\"\\\"\\\"\n",
        "    print(\"=== –ü–†–û–í–ï–†–ö–ê –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫–∏\n",
        "    required_dirs = [\n",
        "        \"data/images/train\", \"data/images/val\",\n",
        "        \"data/labels/train\", \"data/labels/val\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nüìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–ø–æ–∫:\")\n",
        "    for dir_path in required_dirs:\n",
        "        if Path(dir_path).exists():\n",
        "            img_count = len(list(Path(dir_path).glob(\"*.jpg\"))) + len(list(Path(dir_path).glob(\"*.png\")))\n",
        "            txt_count = len(list(Path(dir_path).glob(\"*.txt\")))\n",
        "            print(f\"‚úÖ {dir_path}: {img_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {txt_count} —Ä–∞–∑–º–µ—Ç–æ–∫\")\n",
        "        else:\n",
        "            print(f\"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {dir_path}\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ä–∞–∑–º–µ—Ç–∫–∏\n",
        "    print(\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è:\")\n",
        "    train_missing_labels = 0\n",
        "    train_missing_images = 0\n",
        "    \n",
        "    for img in Path(\"data/images/train\").glob(\"*.jpg\"):\n",
        "        label = Path(\"data/labels/train\") / (img.stem + \".txt\")\n",
        "        if not label.exists():\n",
        "            train_missing_labels += 1\n",
        "    \n",
        "    for label in Path(\"data/labels/train\").glob(\"*.txt\"):\n",
        "        img = Path(\"data/images/train\") / (label.stem + \".jpg\")\n",
        "        if not img.exists():\n",
        "            train_missing_images += 1\n",
        "    \n",
        "    if train_missing_labels > 0:\n",
        "        print(f\"‚ö†Ô∏è  {train_missing_labels} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏ –≤ train\")\n",
        "    if train_missing_images > 0:\n",
        "        print(f\"‚ö†Ô∏è  {train_missing_images} —Ä–∞–∑–º–µ—Ç–æ–∫ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ train\")\n",
        "    \n",
        "    if train_missing_labels == 0 and train_missing_images == 0:\n",
        "        print(\"‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥—É\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ä–∞–∑–º–µ—Ç–∫–∏\n",
        "    print(\"\\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ä–∞–∑–º–µ—Ç–∫–∏:\")\n",
        "    invalid_labels = 0\n",
        "    for label_file in Path(\"data/labels/train\").glob(\"*.txt\"):\n",
        "        try:\n",
        "            with open(label_file) as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) != 5:\n",
        "                        print(f\"‚ùå {label_file.name}:{line_num} - –Ω–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
        "                        invalid_labels += 1\n",
        "                        continue\n",
        "                    \n",
        "                    class_id = int(parts[0])\n",
        "                    if class_id not in [0, 1]:\n",
        "                        print(f\"‚ö†Ô∏è  {label_file.name}:{line_num} - –Ω–µ–≤–µ—Ä–Ω—ã–π –∫–ª–∞—Å—Å {class_id}\")\n",
        "                        invalid_labels += 1\n",
        "                        continue\n",
        "                    \n",
        "                    coords = [float(x) for x in parts[1:]]\n",
        "                    if any(x < 0 or x > 1 for x in coords):\n",
        "                        print(f\"‚ö†Ô∏è  {label_file.name}:{line_num} - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞\")\n",
        "                        invalid_labels += 1\n",
        "                        continue\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ {label_file.name}: {e}\")\n",
        "            invalid_labels += 1\n",
        "    \n",
        "    if invalid_labels == 0:\n",
        "        print(\"‚úÖ –§–æ—Ä–º–∞—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {invalid_labels} –ø—Ä–æ–±–ª–µ–º –≤ —Ä–∞–∑–º–µ—Ç–∫–µ\")\n",
        "    \n",
        "    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    train_imgs = len(list(Path(\"data/images/train\").glob(\"*.jpg\")))\n",
        "    val_imgs = len(list(Path(\"data/images/val\").glob(\"*.jpg\")))\n",
        "    total_imgs = train_imgs + val_imgs\n",
        "    \n",
        "    print(f\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
        "    print(f\"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_imgs}\")\n",
        "    print(f\"Train: {train_imgs} ({train_imgs/total_imgs*100:.1f}%)\")\n",
        "    print(f\"Val: {val_imgs} ({val_imgs/total_imgs*100:.1f}%)\")\n",
        "    \n",
        "    if total_imgs >= 100:\n",
        "        print(\"‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö (–º–∏–Ω–∏–º—É–º 100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)\")\n",
        "    \n",
        "    return total_imgs > 0\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
        "data_ok = check_data_structure()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
