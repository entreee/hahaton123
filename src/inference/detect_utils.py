"""
–ú–æ–¥—É–ª—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è) –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.

–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –î–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
- –û–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
- –î–µ—Ç–µ–∫—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –∫–∞–º–µ—Ä—ã
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import cv2
from ultralytics import YOLO
from pathlib import Path
import numpy as np
from typing import List, Tuple, Optional, Dict
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle


# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
CLASS_NAMES = {
    0: "helmet (–∫–∞—Å–∫–∞)",
    1: "vest (–∂–∏–ª–µ—Ç)"
}

CLASS_COLORS = {
    0: (0, 165, 255),    # –û—Ä–∞–Ω–∂–µ–≤—ã–π –¥–ª—è –∫–∞—Å–∫–∏ (BGR)
    1: (0, 255, 255)     # –ñ–µ–ª—Ç—ã–π –¥–ª—è –∂–∏–ª–µ—Ç–∞ (BGR)
}

CONFIDENCE_THRESHOLD = 0.5


class PPEDetector:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –°–ò–ó.
    """
    
    def __init__(
        self,
        model_path: str,
        conf_threshold: float = CONFIDENCE_THRESHOLD,
        device: str = "auto"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞.
        
        Args:
            model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (.pt)
            conf_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cpu', '0', 'auto')
        """
        self.model_path = Path(model_path)
        self.conf_threshold = conf_threshold
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.model = YOLO(str(model_path))
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device == "auto":
            import torch
            device = "0" if torch.cuda.is_available() else "cpu"
        
        self.device = device
        self.model.to(device)
        
        print(f"‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω: {model_path}")
        print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        print(f"üéØ –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {conf_threshold}")
    
    def detect_image(
        self,
        image_path: str,
        save_result: bool = False,
        output_dir: str = "output/detections",
        show_confidence: bool = True,
        line_thickness: int = 2
    ) -> Tuple[np.ndarray, List[Dict]]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            save_result: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏
            output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            show_confidence: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø–æ–¥–ø–∏—Å–∏
            line_thickness: –¢–æ–ª—â–∏–Ω–∞ –ª–∏–Ω–∏–π bounding box
            
        Returns:
            (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏, —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π)
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        
        original_image = image.copy()
        height, width = image.shape[:2]
        
        # –î–µ—Ç–µ–∫—Ü–∏—è
        results = self.model.predict(
            image_path,
            conf=self.conf_threshold,
            verbose=False,
            device=self.device
        )
        
        detections = []
        boxes = results[0].boxes
        
        if boxes is not None:
            for i, box in enumerate(boxes):
                # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                
                # –ö–ª–∞—Å—Å –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                class_id = int(box.cls[0].cpu().numpy())
                confidence = float(box.conf[0].cpu().numpy())
                
                if confidence >= self.conf_threshold:
                    # –¶–≤–µ—Ç –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
                    color = CLASS_COLORS.get(class_id, (255, 255, 255))  # –ë–µ–ª—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    class_name = CLASS_NAMES.get(class_id, f"class_{class_id}")
                    
                    # –†–∏—Å—É–µ–º bounding box
                    cv2.rectangle(
                        image, (x1, y1), (x2, y2),
                        color, line_thickness
                    )
                    
                    # –ü–æ–¥–ø–∏—Å—å
                    label = f"{class_name}"
                    if show_confidence:
                        label += f": {confidence:.2f}"
                    
                    # –§–æ–Ω –¥–ª—è –ø–æ–¥–ø–∏—Å–∏
                    label_size = cv2.getTextSize(
                        label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                    )[0]
                    cv2.rectangle(
                        image,
                        (x1, y1 - label_size[1] - 10),
                        (x1 + label_size[0], y1),
                        color, -1
                    )
                    
                    # –¢–µ–∫—Å—Ç –ø–æ–¥–ø–∏—Å–∏
                    cv2.putText(
                        image, label,
                        (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6,
                        (255, 255, 255), 2  # –ë–µ–ª—ã–π —Ç–µ–∫—Å—Ç
                    )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ü–∏–∏
                    detections.append({
                        'class_id': class_id,
                        'class_name': class_name,
                        'confidence': confidence,
                        'bbox': [x1, y1, x2, y2],
                        'area': (x2 - x1) * (y2 - y1)
                    })
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if save_result:
            output_path = Path(output_dir) / Path(image_path).name
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            cv2.imwrite(str(output_path), image)
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è BGR -> RGB –¥–ª—è matplotlib
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        return image_rgb, detections
    
    def detect_video(
        self,
        video_path: str,
        output_path: str = None,
        conf_threshold: float = None,
        show_progress: bool = True
    ) -> Path:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–µ.
        
        Args:
            video_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–µ—Å–ª–∏ None - output/detected_video.mp4)
            conf_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç self.conf_threshold)
            show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏
        """
        if conf_threshold is None:
            conf_threshold = self.conf_threshold
        
        # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if output_path is None:
            output_dir = Path("output/videos")
            output_dir.mkdir(parents=True, exist_ok=True)
            video_name = Path(video_path).stem
            output_path = output_dir / f"{video_name}_detected.mp4"
        
        # –û—Ç–∫—Ä—ã—Ç–∏–µ –≤–∏–¥–µ–æ
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"üé¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ: {video_path}")
        print(f"üìè –†–∞–∑–º–µ—Ä: {width}x{height}, FPS: {fps}, –∫–∞–¥—Ä–æ–≤: {total_frames}")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_path}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–ø–∏—Å–∏
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
        
        frame_count = 0
        total_detections = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ –∫–∞–¥—Ä–µ
            results = self.model.predict(
                frame,
                conf=conf_threshold,
                verbose=False,
                device=self.device
            )
            
            # –†–∏—Å—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –∫–æ–¥, —á—Ç–æ –∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
            boxes = results[0].boxes
            frame_detections = 0
            
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                    class_id = int(box.cls[0].cpu().numpy())
                    confidence = float(box.conf[0].cpu().numpy())
                    
                    if confidence >= conf_threshold:
                        color = CLASS_COLORS.get(class_id, (255, 255, 255))
                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                        
                        label = f"{CLASS_NAMES.get(class_id, 'unknown')}"
                        if confidence < 0.9:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö
                            label += f": {confidence:.2f}"
                        
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                        cv2.rectangle(
                            frame, (x1, y1 - label_size[1] - 10),
                            (x1 + label_size[0], y1), color, -1
                        )
                        cv2.putText(
                            frame, label, (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2
                        )
                        
                        frame_detections += 1
                        total_detections += 1
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞–¥—Ä–µ
            cv2.putText(
                frame, f"Frame: {frame_count}/{total_frames}",
                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
            )
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if show_progress and frame_count % 30 == 0:
                progress = frame_count / total_frames * 100
                print(f"  üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({frame_count}/{total_frames})")
            
            # –ó–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞
            out.write(frame)
            frame_count += 1
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        cap.release()
        out.release()
        
        print(f"\n‚úÖ –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ!")
        print(f"üìä –ö–∞–¥—Ä–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {frame_count}")
        print(f"üéØ –î–µ—Ç–µ–∫—Ü–∏–π –Ω–∞–π–¥–µ–Ω–æ: {total_detections}")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        
        return output_path
    
    def detect_camera(
        self,
        camera_id: int = 0,
        conf_threshold: float = None,
        window_name: str = "PPE Detection - Real Time",
        max_frames: Optional[int] = None
    ) -> None:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –∫–∞–º–µ—Ä—ã.
        
        Args:
            camera_id: ID –∫–∞–º–µ—Ä—ã (0 = –æ—Å–Ω–æ–≤–Ω–∞—è –≤–µ–±-–∫–∞–º–µ—Ä–∞)
            conf_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            window_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ–∫–Ω–∞
            max_frames: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ (None = –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ)
        """
        if conf_threshold is None:
            conf_threshold = self.conf_threshold
        
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É {camera_id}")
            print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ camera_id=1")
            return
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–∞–º–µ—Ä—ã
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        print(f"üìπ –ö–∞–º–µ—Ä–∞ –∑–∞–ø—É—â–µ–Ω–∞ ({camera_id})")
        print(f"–ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞, 's' –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞")
        print(f"–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {conf_threshold}")
        
        frame_count = 0
        screenshot_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("‚ö†Ô∏è  –ü–æ—Ç–µ—Ä—è–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–∞–º–µ—Ä–æ–π")
                break
            
            if max_frames and frame_count >= max_frames:
                break
            
            # –î–µ—Ç–µ–∫—Ü–∏—è
            results = self.model.predict(
                frame,
                conf=conf_threshold,
                verbose=False,
                device=self.device
            )
            
            # –†–∏—Å—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏
            boxes = results[0].boxes
            frame_detections = 0
            
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                    class_id = int(box.cls[0].cpu().numpy())
                    confidence = float(box.conf[0].cpu().numpy())
                    
                    if confidence >= conf_threshold:
                        color = CLASS_COLORS.get(class_id, (255, 255, 255))
                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                        
                        label = f"{CLASS_NAMES.get(class_id, 'unknown')}"
                        if confidence < 0.9:
                            label += f": {confidence:.2f}"
                        
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                        cv2.rectangle(
                            frame, (x1, y1 - label_size[1] - 10),
                            (x1 + label_size[0], y1), color, -1
                        )
                        cv2.putText(
                            frame, label, (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2
                        )
                        
                        frame_detections += 1
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–∞ –∫–∞–¥—Ä–µ
            info_y = 30
            cv2.putText(
                frame, f"Frame: {frame_count}", (10, info_y),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
            )
            info_y += 30
            cv2.putText(
                frame, f"Detections: {frame_detections}", (10, info_y),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
            )
            info_y += 30
            cv2.putText(
                frame, "Press 'q' to quit, 's' for screenshot", (10, info_y),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2
            )
            
            # –ü–æ–∫–∞–∑ –∫–∞–¥—Ä–∞
            cv2.imshow(window_name, frame)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞–≤–∏—à
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("üëã –í—ã—Ö–æ–¥ –ø–æ –Ω–∞–∂–∞—Ç–∏—é 'q'")
                break
            elif key == ord('s'):
                # –°–∫—Ä–∏–Ω—à–æ—Ç
                screenshot_dir = Path("output/screenshots")
                screenshot_dir.mkdir(parents=True, exist_ok=True)
                screenshot_path = screenshot_dir / f"screenshot_{frame_count:06d}.jpg"
                cv2.imwrite(str(screenshot_path), frame)
                screenshot_count += 1
                print(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")
                print(f"–í—Å–µ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {screenshot_count}")
            
            frame_count += 1
        
        # –û—á–∏—Å—Ç–∫–∞
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"\nüìπ –î–µ—Ç–µ–∫—Ü–∏—è —Å –∫–∞–º–µ—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"üé¨ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–¥—Ä–æ–≤: {frame_count}")
        print(f"üì∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {screenshot_count}")
    
    def batch_predict(
        self,
        image_folder: str,
        output_folder: str = "output/batch_detections",
        conf_threshold: float = None,
        save_results: bool = True
    ) -> Dict[str, int]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–ø–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.
        
        Args:
            image_folder: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            output_folder: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            conf_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            save_results: –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        if conf_threshold is None:
            conf_threshold = self.conf_threshold
        
        image_path = Path(image_folder)
        output_path = Path(output_folder)
        output_path.mkdir(parents=True, exist_ok=True)
        
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        image_files = []
        for ext in image_extensions:
            image_files.extend(image_path.glob(ext))
            image_files.extend(image_path.glob(ext.upper()))
        
        if len(image_files) == 0:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {image_folder}")
            return {'processed': 0, 'detections': 0, 'errors': 0}
        
        stats = {'processed': 0, 'detections': 0, 'errors': 0}
        
        print(f"üìÇ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_folder}")
        
        for i, image_file in enumerate(image_files, 1):
            try:
                # –î–µ—Ç–µ–∫—Ü–∏—è
                results, detections = self.detect_image(
                    str(image_file),
                    save_result=save_results,
                    output_dir=str(output_path),
                    show_confidence=True
                )
                
                stats['processed'] += 1
                stats['detections'] += len(detections)
                
                if i % 10 == 0:
                    progress = i / len(image_files) * 100
                    print(f"  üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({i}/{len(image_files)})")
            
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_file.name}: {e}")
                stats['errors'] += 1
        
        print(f"\n‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}/{len(image_files)}")
        print(f"üéØ –î–µ—Ç–µ–∫—Ü–∏–π: {stats['detections']}")
        print(f"‚ö†Ô∏è  –û—à–∏–±–æ–∫: {stats['errors']}")
        
        return stats
    
    def get_model_info(self) -> Dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
        """
        try:
            model = YOLO(self.model_path)
            info = {
                'model_path': str(self.model_path),
                'num_classes': len(model.names),
                'class_names': model.names,
                'device': self.device,
                'conf_threshold': self.conf_threshold
            }
            
            # –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ)
            model_size = self.model_path.stat().st_size / (1024*1024)  # MB
            info['model_size_mb'] = round(model_size, 2)
            
            print(f"‚ÑπÔ∏è  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
            print(f"  üìÅ –ü—É—Ç—å: {info['model_path']}")
            print(f"  üì¶ –†–∞–∑–º–µ—Ä: {info['model_size_mb']} MB")
            print(f"  üéØ –ö–ª–∞—Å—Å–æ–≤: {info['num_classes']}")
            print(f"  üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {info['device']}")
            print(f"  üî¢ –ö–ª–∞—Å—Å—ã: {list(info['class_names'].values())}")
            
            return info
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return {}


def visualize_detections(
    image: np.ndarray,
    detections: List[Dict],
    class_names: Dict = CLASS_NAMES,
    class_colors: Dict = CLASS_COLORS,
    figsize: Tuple[float, float] = (12, 8)
) -> plt.Figure:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é matplotlib.
    
    Args:
        image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (RGB –∏–ª–∏ BGR)
        detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
        class_names: –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Å–æ–≤
        class_colors: –°–ª–æ–≤–∞—Ä—å —Ü–≤–µ—Ç–æ–≤
        figsize: –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
        
    Returns:
        Matplotlib —Ñ–∏–≥—É—Ä–∞
    """
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è BGR -> RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if len(image.shape) == 3 and image.shape[2] == 3:
        if image[0, 0, 0] + image[0, 0, 1] + image[0, 0, 2] > 500:  # –í–µ—Ä–æ—è—Ç–Ω–æ BGR
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    ax.imshow(image)
    ax.set_title("–î–µ—Ç–µ–∫—Ü–∏—è –°–ò–ó", fontsize=16, fontweight='bold')
    ax.axis('off')
    
    # –†–∏—Å—É–µ–º bounding boxes
    for detection in detections:
        x1, y1, x2, y2 = detection['bbox']
        class_id = detection['class_id']
        confidence = detection['confidence']
        
        # –¶–≤–µ—Ç –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ
        color = class_colors.get(class_id, 'white')
        class_name = class_names.get(class_id, f'class_{class_id}')
        
        # Bounding box
        rect = Rectangle(
            (x1, y1), (x2 - x1), (y2 - y1),
            linewidth=2, edgecolor=color, facecolor='none',
            alpha=0.7
        )
        ax.add_patch(rect)
        
        # –ü–æ–¥–ø–∏—Å—å
        label = f"{class_name}: {confidence:.2f}"
        ax.text(
            x1, y1 - 5, label,
            bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.7),
            fontsize=10, fontweight='bold', color='white',
            verticalalignment='top'
        )
    
    # –õ–µ–≥–µ–Ω–¥–∞
    legend_elements = [
        plt.Rectangle((0,0),1,1, facecolor=class_colors[0], label=class_names[0]) for i in range(1)
    ] + [
        plt.Rectangle((0,0),1,1, facecolor=class_colors[1], label=class_names[1]) for i in range(1)
    ]
    
    ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))
    
    plt.tight_layout()
    return fig


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    from pathlib import Path
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    detector = PPEDetector("models/ppe_model/weights/best.pt")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    info = detector.get_model_info()
    
    # –¢–µ—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    if Path("data/images/val").exists():
        test_images = list(Path("data/images/val").glob("*.jpg"))
        if test_images:
            result_img, detections = detector.detect_image(str(test_images[0]))
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            fig = visualize_detections(result_img, detections)
            plt.show()
            
            print(f"–ù–∞–π–¥–µ–Ω–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {len(detections)}")
            for det in detections:
                print(f"  {det['class_name']}: {det['confidence']:.2f}")
